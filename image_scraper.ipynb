{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "TODO:\n",
        "\n",
        "*   Some image links are just lead to the website containing the image, need to find a better way to get the full URL of the image (ideally one that doesn't involve messy string manipulation)\n",
        "*   Still results in some broken/inaccesible links, will need to either filter those out in real time or remove them after processing\n",
        "*   Doing \"catch all\" error handling can hide real errors, would like to be able to list and catch specific errors\n",
        "*   Would like to store image URLs along with the keywords their alt contained so that they can be used to help with data cleaning later\n",
        "\n"
      ],
      "metadata": {
        "id": "EOQZ2jCBKdYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dependencies\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from urllib.robotparser import RobotFileParser\n",
        "import random\n",
        "import requests\n",
        "import urllib\n",
        "import time\n",
        "import threading\n",
        "\n",
        "# Google drive integration\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "gu6x-TGAP0jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Scraper():\n",
        "  def __init__(self, PATH_TO_KEYWORDS, PATH_TO_SEED_URLS\n",
        "               ,FOLLOW_ONLY_IF_IMAGES_FOUND = True\n",
        "               ,MAX_TIMEOUT_IN_SECONDS = 3\n",
        "               ,NUM_IMAGES_TO_COLLECT = 1000\n",
        "               ,POLITE_MODE = True\n",
        "               ,THREADS = 10\n",
        "               ,VERBOSE = 1):\n",
        "    # Parameters\n",
        "    self.FOLLOW_ONLY_IF_IMAGES_FOUND = FOLLOW_ONLY_IF_IMAGES_FOUND # Only scrape links on a page if images were actually found on that page\n",
        "    self.MAX_HOST_SELECTION_RETRIES = 5 # Maximum number of times the scraper can attempt to find a new host to scrape if a host has already been recently scraped\n",
        "    self.MAX_TIMEOUT_IN_SECONDS = MAX_TIMEOUT_IN_SECONDS # Maximum timeout for requests\n",
        "    self.NUM_IMAGES_TO_COLLECT = NUM_IMAGES_TO_COLLECT # Total number of image links to collect before the scraper stops\n",
        "    self.PATH_TO_KEYWORDS = PATH_TO_KEYWORDS # Path to file containing keywords split by newlines\n",
        "    self.PATH_TO_SEED_URLS = PATH_TO_SEED_URLS # Path to file containing URLs split by newlines\n",
        "    self.POLITE_MODE = POLITE_MODE # Whether to respect robots.txt or not\n",
        "    self.THREADS = THREADS # Number of threads to create each scrape\n",
        "    self.VERBOSE = VERBOSE # Changes the amount of information shown, should be 0, 1, or 2\n",
        "\n",
        "    # Variable initialization\n",
        "    self.keywords = None # List containing keywords any images downloaded must contain in their alts\n",
        "    self.seed_urls = None # Seed urls for the scraper to start on\n",
        "    self.hosts = dict() # Dictionary sorting URLs by their hosts\n",
        "    self.already_visited = set() # Links that have already been searched\n",
        "    self.image_urls = set() # Scraped image URLs\n",
        "    self.no_scrape_hosts = set() # Don't bother searching URLs under hosts in this set\n",
        "    self.lock = threading.Lock() # Lock used by threading to ensure safety\n",
        "\n",
        "    # State variables\n",
        "    self.loaded_keywords = False\n",
        "    self.loaded_seed_urls = False\n",
        "\n",
        "  def begin_scrape(self):\n",
        "    self._loadData()\n",
        "    self._sortUrlsByHosts()\n",
        "    while len(self.image_urls) < self.NUM_IMAGES_TO_COLLECT:\n",
        "      try:\n",
        "        hosts_to_scrape = self._getHostsToScrape()\n",
        "        self._startScraperThreads(hosts_to_scrape)\n",
        "      except Exception as e: # A variety of errors might be encountered due to incorrectly formatted sites or whatnot\n",
        "        if e is KeyboardInterrupt:\n",
        "          return\n",
        "        self._verboseLog(e, 2)\n",
        "\n",
        "  def _scrape(self, host_to_scrape):\n",
        "    try:\n",
        "      url = self._getRandomUrlFromHost(host_to_scrape)\n",
        "      if self._stopIfScrapingNotAllowed(url, host_to_scrape):\n",
        "        return\n",
        "      parsed = self._parseHTML(url)\n",
        "      num_images_found = self._findAndStoreImages(parsed, url)\n",
        "      if self._shouldContinueSearch(num_images_found):\n",
        "        links = self._getLinks(parsed, url)\n",
        "        self._appendFoundLinksToAppropriateHosts(links)\n",
        "    except Exception as e:\n",
        "        if e is KeyboardInterrupt:\n",
        "          raise e\n",
        "        self._verboseLog(e, 2)\n",
        "\n",
        "  def _startScraperThreads(self, hosts_to_scrape):\n",
        "    threads = []\n",
        "    for host in hosts_to_scrape:\n",
        "      threads.append(threading.Thread(target=self._scrape, args=((host,))))\n",
        "    for thread in threads:\n",
        "      thread.start()\n",
        "    for thread in threads:\n",
        "      thread.join()\n",
        "    self._verboseLog(\"Total image count: \"+str(len(self.image_urls)), 1)\n",
        "\n",
        "  def _getHostsToScrape(self):\n",
        "    hosts_to_scrape = []\n",
        "    already_selected = set()\n",
        "    for i in range(self.THREADS):\n",
        "      attempt = 0\n",
        "      while True:\n",
        "        host = random.choice(list(self.hosts.keys()))\n",
        "        already_selected.add(host)\n",
        "        if host in self.no_scrape_hosts:\n",
        "          del(self.hosts[host])\n",
        "          continue # If this is a no-scrape host, try again\n",
        "        if host in already_selected and attempt < self.MAX_HOST_SELECTION_RETRIES:\n",
        "          attempt += 1\n",
        "          continue # We want to avoid scraping the same host a lot\n",
        "        hosts_to_scrape.append(host)\n",
        "        break\n",
        "    return hosts_to_scrape\n",
        "\n",
        "  def _shouldContinueSearch(self, num_images_found):\n",
        "    if not self.FOLLOW_ONLY_IF_IMAGES_FOUND:\n",
        "      return True\n",
        "    elif num_images_found > 0:\n",
        "      return True\n",
        "    else:\n",
        "      self._verboseLog(\"Website did not yield any images, will not follow links\", 2)\n",
        "      return False\n",
        "\n",
        "  def _appendFoundLinksToAppropriateHosts(self, links):\n",
        "    # Append links to their appropriate hosts\n",
        "      for url in links:\n",
        "        if url in self.already_visited:\n",
        "          continue\n",
        "        if self._link_is_invalid(url):\n",
        "          continue\n",
        "        hostname = urlparse(url).hostname\n",
        "        with self.lock:\n",
        "          if hostname not in self.hosts:\n",
        "            self.hosts[hostname] = []\n",
        "          self.hosts[hostname].append(url)\n",
        "\n",
        "  def _findAndStoreImages(self, parsed, url):\n",
        "    found_imgs = self._getImages(parsed, url)\n",
        "    num_imgs_found = len(found_imgs)\n",
        "    self._verboseLog(\"Found \"+str(num_imgs_found)+\" images\", 2)\n",
        "    with self.lock:\n",
        "      self.image_urls = self.image_urls.union(found_imgs)\n",
        "    return num_imgs_found\n",
        "\n",
        "  def _parseHTML(self, url):\n",
        "    html = requests.get(url, timeout=self.MAX_TIMEOUT_IN_SECONDS).text # this could probably be done with urllib in order to reduce dependencies, but requests is easier (and safer) to use\n",
        "    self._verboseLog(\"Pulling HTML from \" + url, 2)\n",
        "    parsed = BeautifulSoup(html)\n",
        "    return parsed\n",
        "\n",
        "  def _getRandomUrlFromHost(self, host):\n",
        "    index = random.randint(0, len(self.hosts[host])-1)\n",
        "    url = self.hosts[host][index]\n",
        "    return url\n",
        "\n",
        "  def _stopIfScrapingNotAllowed(self, url, host_to_scrape):\n",
        "    if self.POLITE_MODE:\n",
        "      if not self._websiteAllowsScraping(url, host_to_scrape):\n",
        "        self._verboseLog(\"robots.txt doesn't allow scraping, finding a new url to scrape\", 2)\n",
        "        with self.lock:\n",
        "          try:\n",
        "            del(self.hosts[host_to_scrape])\n",
        "          except KeyError:\n",
        "            self._verboseLog(\"Failed to remove a host, maybe it was removed by another thread already?\", 2)\n",
        "          self.no_scrape_hosts.add(host_to_scrape)\n",
        "        return True\n",
        "      return False\n",
        "\n",
        "  def _websiteAllowsScraping(self, url, host_to_scrape):\n",
        "    rp = TimeoutRobotFileParser(timeout=self.MAX_TIMEOUT_IN_SECONDS)\n",
        "    rp.set_url(url)\n",
        "    rp.read()\n",
        "    canFetch = rp.can_fetch(\"*\", host_to_scrape + \"/robots.txt\")\n",
        "    if canFetch:\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  def _sortUrlsByHosts(self):\n",
        "    for url in self.seed_urls:\n",
        "      hostname = urlparse(url).hostname\n",
        "      if hostname not in self.hosts:\n",
        "        self.hosts[hostname] = []\n",
        "      self.hosts[hostname].append(url)\n",
        "\n",
        "  def _loadData(self):\n",
        "    self._loadSeedUrls()\n",
        "    self._loadKeywords()\n",
        "\n",
        "  def _loadKeywords(self):\n",
        "    self.keywords = self._loadFromFile(self.PATH_TO_KEYWORDS)\n",
        "    self.loaded_keywords = True\n",
        "\n",
        "  def _loadSeedUrls(self):\n",
        "    self.seed_urls = self._loadFromFile(self.PATH_TO_SEED_URLS)\n",
        "    self.loaded_seed_urls = True\n",
        "\n",
        "  def _loadFromFile(self, PATH):\n",
        "    with open(PATH, 'r') as f:\n",
        "      txt = f.read()\n",
        "      lst = txt.lower().split('\\n')\n",
        "      # remove trailing newlines if present\n",
        "      lst = self._removeEmptyIndexes(lst)\n",
        "      return lst\n",
        "\n",
        "  def _removeEmptyIndexes(self, l):\n",
        "    l = [elem for elem in l if elem != '']\n",
        "    return l\n",
        "\n",
        "  def _getLinks(self, parsed, url):\n",
        "    links = []\n",
        "    for element in parsed.find_all(\"a\"):\n",
        "      link = element.get(\"href\")\n",
        "      if link == None or link == \"\": # If the link is empty, ignore it\n",
        "        continue\n",
        "      link = self._ensureUniversal(link, url)\n",
        "      links.append(link)\n",
        "    self._verboseLog(\"Collected \"+str(len(links))+\" links\", 2)\n",
        "    return links\n",
        "\n",
        "  def _ensureUniversal(self, link, url):\n",
        "    try:\n",
        "      if link[0] == \"/\" and link[1] == \"/\":\n",
        "        link = link[2:]\n",
        "      elif link[0] == \"/\": # If the link starts with /, append the host url to the beginning to make it a complete URL\n",
        "        if url[-1] == \"/\":\n",
        "          link = url[:-1] + link\n",
        "        else:\n",
        "          link = url + link\n",
        "    except IndexError:\n",
        "      pass # The link is probably broken, just don't do anything and it will get removed later\n",
        "    return link\n",
        "\n",
        "  def _getImages(self, parsed, url):\n",
        "    links = set()\n",
        "    for element in parsed.find_all(\"img\"):\n",
        "      link = element.get(\"src\")\n",
        "      try:\n",
        "        alt = element.get(\"alt\").lower()\n",
        "      except:\n",
        "        continue # If the image doesn't have an alt caption, ignore it\n",
        "      # Ensure that keywords are present in alt\n",
        "      keywords_present = False\n",
        "      for keyword in self.keywords:\n",
        "        if keyword in alt:\n",
        "          keywords_present = True\n",
        "      if not keywords_present:\n",
        "        continue\n",
        "      if link == None or link == \"\": # If the link is empty, ignore it\n",
        "        continue\n",
        "      link = self._ensureUniversal(link, url)\n",
        "      links.add(link)\n",
        "    return links\n",
        "\n",
        "  def _link_is_invalid(self, x): # Taken from https://stackoverflow.com/a/38020041\n",
        "    try:\n",
        "        result = urlparse(x)\n",
        "        return not all([result.scheme, result.netloc])\n",
        "    except AttributeError:\n",
        "        return True\n",
        "\n",
        "  def _verboseLog(self, msg, lvl):\n",
        "    if self.VERBOSE >= lvl:\n",
        "      print(msg)\n"
      ],
      "metadata": {
        "id": "BQSGtsKSL5R4"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Taken from https://stackoverflow.com/a/15235475\n",
        "# Subclass with override of read method that allows for a shorter timout\n",
        "class TimeoutRobotFileParser(RobotFileParser):\n",
        "    def __init__(self, url='', timeout=60):\n",
        "        super().__init__(url)\n",
        "        self.timeout = timeout\n",
        "\n",
        "    def read(self):\n",
        "        \"\"\"Reads the robots.txt URL and feeds it to the parser.\"\"\"\n",
        "        try:\n",
        "            f = urllib.request.urlopen(self.url, timeout=self.timeout)\n",
        "        except urllib.error.HTTPError as err:\n",
        "            if err.code in (401, 403):\n",
        "                self.disallow_all = True\n",
        "            elif err.code >= 400:\n",
        "                self.allow_all = True\n",
        "        else:\n",
        "            raw = f.read()\n",
        "            self.parse(raw.decode(\"utf-8\").splitlines())"
      ],
      "metadata": {
        "id": "hwnhm90_aVop"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KEYWORDS_PATH = \"gdrive/MyDrive/Programming/Web Scraper/keywords.txt\"\n",
        "SEED_URLS_PATH = \"gdrive/MyDrive/Programming/Web Scraper/seed-urls.txt\"\n",
        "scraper = Scraper(KEYWORDS_PATH, SEED_URLS_PATH)"
      ],
      "metadata": {
        "id": "HIbK7crlFyCT"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scraper.begin_scrape()"
      ],
      "metadata": {
        "id": "4FHA5f_DMV4f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
